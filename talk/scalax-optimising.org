#+TITLE: Optimising Scala for fun and profit
#+AUTHOR: Rory Graves
#+DATE: Scala eXchange 2015

#+TODO: TODO | RESEARCH | NOTES | CHART | DIAGRAM | DRAWING | CODE | VIDEO

* Lets start with an example!

** Knuth

#+BEGIN_QUOTE
*Premature* optimization is the root of all evil -- Donald Knuth
#+END_QUOTE

- Premature is the keyword
- Be pragmatic!

** TL;DR;

It's the scientific method

1. Observe
2. Hypothesis
3. Test
4. Repeat

** Debugging logging

#+BEGIN_NOTES
From real world experience
#+END_NOTES

Lets take a critical loop in our program.

#+BEGIN_SRC scala
  def myHardworkingFunction(): Int = {
    var x = 0

    for(i <- 1 to limit) {
      logDebug(s"started iteration $i")
      x += i // some maths type work
      logDebug(s"finished iteration $i")
    }

    x
  }
#+END_SRC

** JMH Benchmark

#+BEGIN_SRC scala
  @Benchmark
  def basicLogging(): Unit = {
    var x = 0
    for(i <- 1 to limit) {
      logDebug(s"started iteration $i")
      x += i
      logDebug(s"finished iteration $i")
    }
    if(debugEnabled)
      tmp = x
  }
#+END_SRC

** Definitions

#+BEGIN_SRC scala
object LazyConfig {
  val limit = 1000000  // 1M
  val debugEnabled = Option(System.getProperty("DEBUG")).isDefined
  var tmp = 0
}
#+END_SRC

#+BEGIN_SRC scala
  def logDebug(s: String): Unit = {
    if(debugEnabled)
      println(s)
  }
#+END_SRC

** Results

#+BEGIN_SRC
[info] Result "basicLogging":
[info]   4.813 ±(99.9%) 0.056 ops/s [Average]
[info]   (min, avg, max) = (4.577, 4.813, 4.888), stdev = 0.065
[info]   CI (99.9%): [4.756, 4.869] (assumes normal distribution)
#+END_SRC

- So ~200ms/run
- 8,000,004 Objects created
  - 4M WrappedArray$ofRef
  - 2M StringBuilders
  - 2M StringContext$$anonfun$s$1
  - Range$Inclusive
  - Range
  - LazyLogging$$anonfun$basicLogging$1 (closure)
  - IntRef

** Lets go lazy!


#+BEGIN_SRC scala
  def lazyLogDebug(f: => String): Unit = {
    if(debugEnabled)
      println(f)
  }
#+END_SRC

** Lazy results

#+BEGIN_SRC scala
[info] Result "lazyLogging":
[info]   142.139 ±(99.9%) 9.268 ops/s [Average]
[info]   (min, avg, max) = (125.545, 142.139, 158.320), stdev = 10.673
[info]   CI (99.9%): [132.871, 151.408] (assumes normal distribution)
[info]
#+END_SRC

- ~30x faster
- 2,000,004 objects created
  - 1M LazyLogging$$...$sp$1
  - 1M LazyLogging$$...$sp$2
  - Range$Inclusive
  - Range
  - LazyLogging$$anonfun$basicLogging$1 (closure)
  - IntRef

** Guards
#+BEGIN_SRC scala
  @Benchmark
  def guardedLogging(): Unit = {
    var x = 0
    for(i <- 1 to limit) {
      if(debugEnabled)
        logDebug(s"started iteration $i")
      x += i
      if(debugEnabled)
        logDebug(s"finished iteration $i")
    }

    if(debugEnabled)
      tmp = x
  }
#+END_SRC

** Guarded results

#+BEGIN_SRC scala
[info] Result "guardedLogging":
[info]   1624.764 ±(99.9%) 88.435 ops/s [Average]
[info]   (min, avg, max) = (1416.897, 1624.764, 1783.305), stdev = 101.842
[info]   CI (99.9%): [1536.328, 1713.199] (assumes normal distribution)
[info]
#+END_SRC

- ~430x faster
- 4 Objects created per iteration
  - Range$Inclusive
  - Range
  - LazyLogging$$anonfun$basicLogging$1 (closure)
  - IntRef

** Can we do better?

** Yes!

#+BEGIN_SRC scala
    var i = 1
    while(i <= limit) {
      if(debugEnabled)
        logDebug(s"started iteration $i")
      x += i
      if(debugEnabled)
        logDebug(s"finished iteration $i")
      i += 1
    }
#+END_SRC

** Results

#+BEGIN_SRC
[info] Result "noForLoopGuarded":
[info]   3509.757 ±(99.9%) 53.530 ops/s [Average]
[info]   (min, avg, max) = (3351.090, 3509.757, 3585.212), stdev = 61.645
[info]   CI (99.9%): [3456.227, 3563.286] (assumes normal distribution)
#+END_SRC

- ~750x faster
- No object creation.
- A bit ugly
  - but we can fix that
  - https://github.com/nativelibs4java/Scalaxy

** Summary


| Benchmark          | Operations/Sec | Objects created | Vs basic logging |
|--------------------+----------------+-----------------+------------------|
| basicLogging       | 4.831    | 8,000,004 | 1    |
| noLogging          | 2103.904 | 4          | ~430X |
| lazyLogging        | 157.277  | 2,000,004  | ~32X|
| guardedLogging     | 2111.598 | 4          | ~430 |
| while loop guarded | 3615.795 | 0          | ~750 |

- Naive code can be slow
- Naive fixes can be slow

** Microbenchmarks are flawed

- In the real world Great if you already know whats slow
- Benchmarks interaction with:
  - JIT
  - Dead code removal.
  - caches

* Now for the Introduction!


** Rory Graves =@a_dev_musing=

#+BEGIN_NOTES

Actors before they were cool.

Ensime Lightening talk is straight after this talk in this room.

#+END_NOTES

- PhD in Computer Science
    - Active Networks
    - Wrote a JVM
- Wrote mobile games before it was cool
- Shows people around an old windmill at weekends
- ENSIME core developer
- Spent far too long seeing every example in this code.

* What the rest of the talk says!

** Outline

- The things I'm not going to talk about
- Basic information
- Spotting performance issues
  - once you understand them fixing them is normally easy
- Examples from the field

* What I am not going to talk about...

** Microbenchmarks

- Useful in a subset of cases
- This talk focuses on big picture optimisations

** Mechanical Sympathy

- Ordering to reduce processor cache misses
- loop unrolling
- branch predition

** Multithreading

- Switching overhead
- Complexity overhead
- Simple things can be done faster on one thread

** Real numbers

- Exact numbers depend on too many factors, only comparisons are useful
  - cpu
  - memory
  - compiler code generation
  - JIT optimisation

** Big O

- I lied!
- But I'm not going into details
- Nothing here beats choosing the right algorithm
- But optimisation can make a too slow problem good enough
- https://www.coursera.org/course/algo

* Basic information

** CPU performance.

- CPUs are fast
- no, really fast.
- no, really really fast!
- Intel Core i7 5960X -	238,310 MIPS at 3.0 GHz
- Ensure they do useful work, keep them fed.

** Latency cheatsheet

#+BEGIN_SRC
Latency Comparison Numbers
--------------------------
L1 cache reference                            0.5 ns
Branch mispredict                             5   ns
L2 cache reference                            7   ns             14x L1 cache
Mutex lock/unlock                            25   ns
Main memory reference                       100   ns             20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy              3,000   ns
Send 1K bytes over 1 Gbps network        10,000   ns    0.01 ms
Read 4K randomly from SSD*              150,000   ns    0.15 ms
Read 1 MB sequentially from memory      250,000   ns    0.25 ms
Round trip within same datacenter       500,000   ns    0.5  ms
Read 1 MB sequentially from SSD*      1,000,000   ns    1    ms  4X memory
Disk seek                            10,000,000   ns   10    ms  20x datacenter roundtrip
Read 1 MB sequentially from disk     20,000,000   ns   20    ms  80x memory, 20X SSD
Send packet CA->Netherlands->CA     150,000,000   ns  150    ms
#+END_SRC

https://gist.github.com/jboner/2841832

** Tools

- jhm
- jvisualvm
- YourKit (free OpenSource license)
- Common sense

** The hidden cost of GC

- Functional programming tends to create lots of short term objects
- Creating objects is expensive
  - Work to create
  - work to free
- Object creation and lifecycle management is not free


* Benchmarking and Analysis



** Holistic approach

- Generally if you have a performance problem





** Lets code!



#+BEGIN_NOTES
#+END_NOTES

#+BEGIN_SRC scala
package brucelee.api {
  sealed trait Receptacle
  case class Glass(a: String) extends Receptacle
  case class Bottle(a: Int) extends Receptacle
  case class Teapot(a: Boolean) extends Receptacle
}
#+END_SRC

#+BEGIN_SRC scala
package brucelee.format {
  object MyFormats extends FamilyFormats {
    implicit override def eitherFormat[A, B](implicit
      a: JsonFormat[A],
      b: JsonFormat[B]) = super.eitherFormat[A, B]
    implicit val symbolFormat = SymbolJsonFormat

    implicit val ReceptacleF: JsonFormat[Receptacle] = cachedImplicit
  }
}
#+END_SRC


* Question Time!

Thanks for listening!

Rory Graves (@a_dev_musing)
